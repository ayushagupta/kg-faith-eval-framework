# Usage

```
~$ pwd
LLM-Reasoning-Benchmark/Code
~$ python -m faitheval.evaluate --input_path <path_to_input_file> --output_path <path_to_output_file> --hallucination_log_path <path_to_output_hallucination_log_json_file>
```

Input file must be in expected JSON format - see `LLM-Reasoning-Benchmark/Code/sample_data/faitheval_example_input.json`